---
title: A Short Note on Support Vector Machine (Part 1)
subtitle: From Motivation, Formulation to Standard Quadratic Programming
author: Wenbo
date: '2018-01-30'
---



<p><strong>Introduction</strong></p>
<p>This is part of my self-note on understanding the derivation of support vector machine (SVM). This short note aims to answer the following question</p>
<ul>
<li>What motivats the SVM</li>
<li>How we formulate the motivation into an mathematical optimization problem</li>
<li>How to simplify the original optimization problem and solve it by Quadratic Programming (QP)</li>
</ul>
<p>This short note is based on the understanding of Prof. Hsuan-Tien Lin’s Video on Machine Learning Techniques</p>
<p><strong>Motivation</strong></p>
<center>
<img src="/img/svm/svm1.png" alt="Figure is from Prof. Hsuan-Tien Lin’s Slides on ML Techniques" style="width:40.0%" /> Figure is from Prof. Hsuan-Tien Lin’s Slides on ML Techniques
</center>
<p>Given the three plots above, there are three classifiers all can separate the positive and negative case. If you can choose one from the three, which one do you want? I guess most of the people will choose the third one. At least I will choose the third one because it is robust to small perturbation of the data. (Think of a scenario that x,y axis represents people’s height and weight. It is highly possible that the data is noisy, say, sometimes you report your height as 184cm or 185cm. In this case a robust(stable) classifier will still make right prediction)</p>
<p><strong>Formulate the Question in Plain English</strong></p>
<p>To achieve such goal above, we have to define what is a good classifier. In SVM, we firstly define margin as the smallest distance between any point and the decision boundary. Then we could say a classifier is better than the other if the its margin is larger than the others’ (Intuitively it means all points can at least perturbate the margin without change in prediction). Another requirement is the usual one that all prediction must be correct. Therefore we can formulate a maximization problem as follows:</p>
<center>
<strong>max</strong>: margin<br />
<strong>s.t.</strong> margin defined as minimum distance(sample point i, boundary line/hyperplane)<br />
every sample is correctly classified
</center>
<p><strong>Formulate/Translate the Question into Mathematics</strong></p>
<p>In order to solve the optimization problem using mathematics, we have to firstly translate it into mathematics (modeling).</p>
<p>Before that , we have to equip ourselves with some geometry knowledge.</p>
<ul>
<li>A hyper plane in <span class="math inline">\(R^{n}\)</span> can be expressed as (w,b) where <span class="math inline">\(w^{T}x+b = 0\)</span></li>
<li>A line perpendicular to this plane can be expressed as <span class="math inline">\(w^{T}\)</span></li>
<li>The distance between a point <span class="math inline">\(x_{i}\)</span> in <span class="math inline">\(r^{n}\)</span> and the hyperplane can be expressed as <span class="math inline">\(\frac{1}{\|w\|} |w^{T}x_{i}+b|\)</span></li>
</ul>
<p>Given sample point is denoted <span class="math inline">\(x_{i}\)</span> and class is <span class="math inline">\(y_{i} \in \{-1,+1\}\)</span>, the above optimization problem can be translated as</p>
<center>
max <span class="math inline">\(min_{i} \frac{1}{\|w\|} |w^{T}x_{i}+b|\)</span><br />
s.t. <span class="math inline">\(y_{i}(w^{T}x_{i}+b)&gt;0\)</span>
</center>
<p>The above problem can be simplified (and equivalent)</p>
<center>
max <span class="math inline">\(min_{i} \frac{1}{\|w\|} y_{i}(w^{T}x_{i}+b)\)</span><br />
s.t. <span class="math inline">\(y_{i}(w^{T}x_{i}+b)&gt;0\)</span>
</center>
<p>because if <span class="math inline">\(y_{i}(w^{T}x_{i}+b)&gt;0\)</span> holds, then <span class="math inline">\(y_{i}(w^{T}x_{i}+b)=|y_{i}(w^{T}x_{i}+b)|=|w^{T}x_{i}+b|\)</span> given <span class="math inline">\(y \in \{-1,1\}\)</span></p>
<p><strong>Narrow our goal to Simplify the Question</strong></p>
<p>The above maxmium minimum problem is still complicated. By taking a closer look at the problem, we can find if (<span class="math inline">\(w^{*},b^{*}\)</span>) is a solution, then (<span class="math inline">\(kw^{*},kb^{*}\)</span>) is also the solution. (i.e. satisfying all constraints and achieves the same optimal value for target function). Therefore, we narrow our goal that we are only interested in the solution making <span class="math inline">\(min_{i} (w^{T}x_{i}+b)=1\)</span>. To illustrate the logic, let’s say we assume (<span class="math inline">\(w^{*},b^{*}\)</span>) is an optimal solution making <span class="math inline">\(min_{i} (w^{T}x_{i}+b)=M\)</span>. By scaling <span class="math inline">\(w^{*},b^{*}\)</span> we can always have a new solution <span class="math inline">\(\frac{w^{*}}{M}, \frac{b^{*}}{M}\)</span> making <span class="math inline">\(min_{i} (w^{T}x_{i}+b)=1\)</span>. Therefore the narrowed version only explores that sort of solution space. And consequently the narrowed version can be much simplified as</p>
<center>
max <span class="math inline">\(\frac{1}{\|w\|}\)</span><br />
s.t. min <span class="math inline">\(y_{i}(w^{T}x_{i}+b)=1\)</span>
</center>
<p>which is equivalent to</p>
<center>
min <span class="math inline">\(\frac{1}{2}{w^{T}w}\)</span><br />
s.t. min <span class="math inline">\(y_{i}(w^{T}x_{i}+b)=1\)</span>
</center>
<p>after the following treatment (all of them preserve the equivalence)</p>
<ul>
<li>maximization becomes minimization reciprocal,</li>
<li>adding <span class="math inline">\(\frac{1}{2}\)</span></li>
<li>minimize <span class="math inline">\(\|w\|\)</span> is equivalent to minimize <span class="math inline">\(\|w\|^{2}=w^{t}w\)</span></li>
</ul>
<p><strong>Relaxation</strong></p>
<p>Unfortunately, the current optimization problem is still hard to solve (due to minimization in constraints). We therefore have to relax the constraint (remove minimization) to <span class="math inline">\(y_{i}(w^{T}x_{i}+b_{i})&gt;=1\)</span>. To illustrate the equivalence of this relaxation. Assume under the new relaxed constraint, we achieve a solution <span class="math inline">\(w^{&#39;},b^{&#39;}\)</span> and all <span class="math inline">\(y_{i}(w^{T}x_{i}+b_{i})&gt;1\)</span>. By scaling invariance (illustrated above), we can easily get a new <span class="math inline">\((\frac{w^{&#39;}}{k},\frac{b^{&#39;}}{k})\)</span> making a smaller <span class="math inline">\(\frac{1}{2}{\|w\|}\)</span>. Therefore <span class="math inline">\(w^{&#39;},b^{&#39;}\)</span> when all <span class="math inline">\(y_{i}(w^{T}x_{i}+b_{i})&gt;1\)</span> cannot be an optimal solution. There has to be a <span class="math inline">\(y_{i}(w^{T}x_{i}+b_{i})=1\)</span>. Hence the relaxation is equivalent to the original problem.</p>
<p><strong>Solving the Probelm by Quadratic Programming</strong></p>
<p>Now current optimization problem after relaxation is stated as follows:</p>
<center>
min <span class="math inline">\(\frac{1}{2}{w^{T}w}\)</span><br />
s.t. <span class="math inline">\(y_{i}(w^{T}x_{i}+b)&gt;=1\)</span>
</center>
<p>The above is a Quadratic Programming problem which can be easily solved with a QP solver.</p>
<center>
<img src="/img/svm/svm2.png" alt="Figure is from Prof. Hsuan-Tien Lin’s Slides on ML Techniques" style="width:40.0%" /><br />
Figure is from Prof. Hsuan-Tien Lin’s Slides on ML Techniques
</center>
